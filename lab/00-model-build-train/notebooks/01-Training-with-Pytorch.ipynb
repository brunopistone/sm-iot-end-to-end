{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Inference of our ML Model\n",
    "\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "In this exercise you will do:\n",
    " - Run a Preprocessing Job using Amazon SageMaker Processing Job\n",
    " - Run a Pytorch Training Job using Amazon SageMaker Training Job\n",
    " - Run a Batch Inference Job using Amazon SageMaker Batch Transform\n",
    " - Compute the thresholds, used by the applicatio to classify the predictions as anomalies or normal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1/3 - Setup\n",
    "Here we'll import some libraries and define some variables. You can also take a look on the scripts that were previously created for preparing the data and training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "bucket_name = \"\"\n",
    "\n",
    "prefix = \"data\"\n",
    "\n",
    "sagemaker_session=sagemaker.Session(default_bucket=bucket_name)\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataset and upload it to an S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the \n",
    "# clean the buckets first\n",
    "s3_client.delete_object(Bucket=bucket_name, Key=\"{}/input\".format(prefix))\n",
    "\n",
    "input_data = sagemaker_session.upload_data('./../data/dataset_wind_turbine.csv.gz', key_prefix=\"{}/input\".format(prefix) )\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training script & the preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script was created to express what we saw in the previous exercise.\n",
    "## It will get the raw data from the turbine sensors, select some features, \n",
    "## denoise, normalize, encode and reshape it as a 6x10x10 tensor\n",
    "## This script is the entrypoint of the first step of the ML Pipelie: Data preparation\n",
    "!pygmentize ./../algorithms/preprocessing/preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the training/prediction script, used by the training step of \n",
    "## our ML Pipeline. In this step, a SageMaker Training Job will run this \n",
    "## script to build the model. Then, in the batch transform step,\n",
    "## the same script will be used again to load the trained model\n",
    "## and rebuild (predict) all the training samples. These predictions\n",
    "## will then be used to compute MAE and the thresholds, for detecting anomalies\n",
    "!pygmentize ./../algorithms/training/wind_turbine.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2/3: Run the end to end ML workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import logging\n",
    "import sagemaker\n",
    "from sagemaker.inputs import CreateModelInput, TrainingInput, TransformInput\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import CacheConfig, CreateModelStep, ProcessingStep, TrainingStep, TransformStep\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1/4: Create the Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = \"s3://{}/{}/input\".format(bucket_name, prefix)\n",
    "\n",
    "preprocessing_framework_version = \"0.23-1\"\n",
    "preprocessing_instance_type = \"ml.m5.xlarge\"\n",
    "preprocessing_instance_count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session=sagemaker.Session(default_bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor = SKLearnProcessor(\n",
    "    framework_version=preprocessing_framework_version,\n",
    "    role=role,\n",
    "    instance_type=preprocessing_instance_type,\n",
    "    instance_count=preprocessing_instance_count,\n",
    "    max_runtime_in_seconds=7200,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = script_processor.run(\n",
    "    code=\"./../algorithms/preprocessing/preprocessing.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination='/opt/ml/processing/input')\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='train_data', \n",
    "            source='/opt/ml/processing/train',\n",
    "            destination='s3://{}/{}/output/train_data'.format(bucket_name, prefix)),\n",
    "        ProcessingOutput(\n",
    "            output_name='statistics', \n",
    "            source='/opt/ml/processing/statistics',\n",
    "            destination='s3://{}/{}/output/statistics'.format(bucket_name, prefix))\n",
    "    ],\n",
    "    arguments=['--num-dataset-splits', '20']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2/4: Create the Trining Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_framework_version = \"1.6.0\"\n",
    "training_python_version = \"py3\"\n",
    "training_instance_type = \"ml.c5.4xlarge\"\n",
    "training_instance_count = 1\n",
    "training_hyperparameters = {\n",
    "    'k_fold_splits': 6,\n",
    "    'k_index_only': 3, # after running some experiments with this dataset, it makes sense to fix it\n",
    "    'num_epochs': 20,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 0.0001,\n",
    "    'dropout_rate': 0.001\n",
    "}\n",
    "training_metrics = [\n",
    "    {'Name': 'train_loss:mse', 'Regex': ' train_loss=(\\S+);'},\n",
    "    {'Name': 'test_loss:mse', 'Regex': ' test_loss=(\\S+);'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session=sagemaker.Session(default_bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "        './../algorithms/training/wind_turbine.py',\n",
    "        framework_version=training_framework_version,\n",
    "        role=role,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        instance_type=training_instance_type,\n",
    "        instance_count=training_instance_count,\n",
    "        py_version=training_python_version,\n",
    "        hyperparameters=training_hyperparameters,\n",
    "        metric_definitions=training_metrics,\n",
    "        output_path=\"s3://{}/models\".format(bucket_name)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\"train\": TrainingInput(\n",
    "        s3_data=\"s3://{}/{}/output/train_data\".format(bucket_name, prefix),\n",
    "        content_type=\"application/x-npy\"\n",
    "    )}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3/4: Register Model in the Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = \"mlops-iot-package-group\"\n",
    "model_approval_status = \"PendingManualApproval\"\n",
    "\n",
    "transform_instance_type = \"ml.c5.xlarge\"\n",
    "transform_instance_count = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Package Group Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_response = sm_client.describe_model_package_group(\n",
    "    ModelPackageGroupName=model_package_group_name\n",
    ")\n",
    "\n",
    "print(describe_response)\n",
    "\n",
    "if describe_response == \"\":\n",
    "    response = sm_client.create_model_package_group(\n",
    "        ModelPackageGroupName=model_package_group_name\n",
    "    )\n",
    "    \n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Trained Model in the Model Package Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.register(\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    content_types=[\"application/x-npy\"],\n",
    "    response_types=[\"application/x-npy\"],\n",
    "    inference_instances=[transform_instance_type],\n",
    "    transform_instances=[transform_instance_type]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4/4: Run Batch Transform Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_instance_type = \"ml.c5.xlarge\"\n",
    "transform_instance_count = 2\n",
    "\n",
    "output_batch_data = \"s3://{}/{}/output/eval\".format(bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Batch Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(\n",
    "    instance_count=transform_instance_count,\n",
    "    instance_type=transform_instance_type,\n",
    "    strategy=\"MultiRecord\",\n",
    "    assemble_with=\"Line\",\n",
    "    output_path=output_batch_data,\n",
    "    accept='application/x-npy',\n",
    "    max_payload=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.transform(\n",
    "    \"s3://{}/{}/output/train_data\".format(bucket_name, prefix), \n",
    "    content_type=\"application/x-npy\", \n",
    "    split_type=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3/3 - Compute the threshold based on MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the predictions & Compute MAE/thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "input_data = sm_client.describe_training_job(TrainingJobName=estimator._current_job_name)\n",
    "input_data = input_data['InputDataConfig'][0]['DataSource']['S3DataSource']['S3Uri']\n",
    "\n",
    "tokens = input_data.split('/', 3)\n",
    "sagemaker_session.download_data(bucket=bucket_name, key_prefix='data/output/eval/', path='./../data/preds/')\n",
    "sagemaker_session.download_data(bucket=bucket_name, key_prefix=tokens[3], path='./../data/input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "x_inputs = np.vstack([np.load(i) for i in glob.glob('./../data/input/*.npy')])\n",
    "y_preds = np.vstack([np.load(i) for i in glob.glob('./../data/preds/*.out')])\n",
    "\n",
    "n_samples,n_features,n_rows,n_cols = x_inputs.shape\n",
    "\n",
    "x_inputs = x_inputs.reshape(n_samples, n_features, n_rows*n_cols).transpose((0,2,1))\n",
    "y_preds = y_preds.reshape(n_samples, n_features, n_rows*n_cols).transpose((0,2,1))\n",
    "\n",
    "mae_loss = np.mean(np.abs(y_preds - x_inputs), axis=1).transpose((1,0))\n",
    "mae_loss[np.isnan(mae_loss)] = 0\n",
    "\n",
    "thresholds = np.mean(mae_loss, axis=1)\n",
    "\n",
    "if not(os.path.exists(\"./../data/statistics\")):\n",
    "    os.mkdir(\"./../data/statistics\")\n",
    "\n",
    "np.save('./../data/statistics/thresholds.npy', thresholds)\n",
    "print(\",\".join(thresholds.astype(str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Now, you can start the next exercise which is on creating an Amazon SageMaker Pipeline for autimating all the steps defined in this notebook.\n",
    "\n",
    "1. [__Training Pipeline__](./02-SageMaker-Pipeline-Training.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
